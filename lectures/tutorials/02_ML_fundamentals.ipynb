{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7811dc1d-c6c3-460e-b073-ea6e1955113e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f2995-151e-4b9d-be4c-343f7bc8dc77",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Tutorial 2\n",
    "\n",
    "UBC 2025-26\n",
    "\n",
    "## Outline\n",
    "\n",
    "During this tutorial, we will focus on the ideas of generalization, training, validation and test scores, and cross-validation. Additionally, we will play with different algorithms and see how their hyperparameters affect their complexity.\n",
    "\n",
    "All questions can be discussed with your classmates and the TAs - this is not a graded exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5db5d-5339-4a9c-9343-824e3325a3d7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69677356-d886-42f0-9f80-db95323471dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
    "# from plotting_functions import *\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"..\"), \"data/\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef33d48-69f6-4f2a-8f8d-be05172954a0",
   "metadata": {},
   "source": [
    "We are going to practice ML fundamentals on a dataset about building characteristics and energy consumption. You may download the dataset [here](https://www.kaggle.com/datasets/govindaramsriram/energy-consumption-dataset-linear-regression?select=train_energy_data.csv) (make sure to download the file called `training_energy_data.csv`. At this page, you will also find information about the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65141e00-6157-4cfe-bc13-4fb314e60ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Building Type</th>\n",
       "      <th>Square Footage</th>\n",
       "      <th>Number of Occupants</th>\n",
       "      <th>Appliances Used</th>\n",
       "      <th>Average Temperature</th>\n",
       "      <th>Day of Week</th>\n",
       "      <th>Energy Consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Residential</td>\n",
       "      <td>7063</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>29.84</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2713.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>44372</td>\n",
       "      <td>66</td>\n",
       "      <td>45</td>\n",
       "      <td>16.72</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>5744.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Industrial</td>\n",
       "      <td>19255</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>14.30</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>4101.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Residential</td>\n",
       "      <td>13265</td>\n",
       "      <td>14</td>\n",
       "      <td>41</td>\n",
       "      <td>32.82</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>3009.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>13375</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>11.92</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>3279.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Residential</td>\n",
       "      <td>14419</td>\n",
       "      <td>68</td>\n",
       "      <td>44</td>\n",
       "      <td>23.95</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>3661.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Industrial</td>\n",
       "      <td>12194</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>14.67</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>3546.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>39562</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>32.18</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>5147.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Residential</td>\n",
       "      <td>8348</td>\n",
       "      <td>67</td>\n",
       "      <td>37</td>\n",
       "      <td>16.48</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>3244.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>15813</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>31.40</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>3423.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Building Type  Square Footage  Number of Occupants  Appliances Used  \\\n",
       "0     Residential            7063                   76               10   \n",
       "1      Commercial           44372                   66               45   \n",
       "2      Industrial           19255                   37               17   \n",
       "3     Residential           13265                   14               41   \n",
       "4      Commercial           13375                   26               18   \n",
       "..            ...             ...                  ...              ...   \n",
       "995   Residential           14419                   68               44   \n",
       "996    Industrial           12194                    7               22   \n",
       "997    Commercial           39562                   88               20   \n",
       "998   Residential            8348                   67               37   \n",
       "999    Commercial           15813                   57               11   \n",
       "\n",
       "     Average Temperature Day of Week  Energy Consumption  \n",
       "0                  29.84     Weekday             2713.95  \n",
       "1                  16.72     Weekday             5744.99  \n",
       "2                  14.30     Weekend             4101.24  \n",
       "3                  32.82     Weekday             3009.14  \n",
       "4                  11.92     Weekday             3279.17  \n",
       "..                   ...         ...                 ...  \n",
       "995                23.95     Weekend             3661.21  \n",
       "996                14.67     Weekend             3546.34  \n",
       "997                32.18     Weekday             5147.21  \n",
       "998                16.48     Weekend             3244.98  \n",
       "999                31.40     Weekend             3423.63  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_df = pd.read_csv(DATA_DIR + 'train_energy_data.csv')\n",
    "energy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a14e91",
   "metadata": {},
   "source": [
    "We are going to try to predict the energy consumption of a building given its characteristics - this is a **regression** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68149c4c-0a63-4e6f-b3a2-8fe32157f6ef",
   "metadata": {},
   "source": [
    "## EDA: Exploratory Data Analysis\n",
    "\n",
    "### <font color='red'>Question 1</font>\n",
    "\n",
    "Let's start answering some questions about this dataset:\n",
    "\n",
    "- How many samples are included in the dataset?\n",
    "- Are the columns the correct type (strings as strings, numbers as numbers)? Do we have missing data?\n",
    "- Looking at the column names (and [descriptions](https://www.kaggle.com/datasets/abrambeyer/openintro-possum/data)), do you think some of them would be more or less helpful in predicting the building energy consumption?\n",
    "- What is the square footage of the largest building in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d6c070-0031-46b7-b300-66beaa646b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many data points do we have? \n",
    "energy_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda2c2a5-aaab-441b-a340-461598b79eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Building Type        1000 non-null   object \n",
      " 1   Square Footage       1000 non-null   int64  \n",
      " 2   Number of Occupants  1000 non-null   int64  \n",
      " 3   Appliances Used      1000 non-null   int64  \n",
      " 4   Average Temperature  1000 non-null   float64\n",
      " 5   Day of Week          1000 non-null   object \n",
      " 6   Energy Consumption   1000 non-null   float64\n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# What is the type and count for each column?\n",
    "energy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31509f7-d455-48a7-a89e-1c515f28aade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Building Type', 'Square Footage', 'Number of Occupants',\n",
       "       'Appliances Used', 'Average Temperature', 'Day of Week',\n",
       "       'Energy Consumption'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the columns in the dataset? \n",
    "energy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b50f5be-2d16-468b-8e5d-ea27521ec7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Square Footage</th>\n",
       "      <th>Number of Occupants</th>\n",
       "      <th>Appliances Used</th>\n",
       "      <th>Average Temperature</th>\n",
       "      <th>Energy Consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25462.388</td>\n",
       "      <td>48.372000</td>\n",
       "      <td>25.606000</td>\n",
       "      <td>22.611390</td>\n",
       "      <td>4166.252570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14294.554</td>\n",
       "      <td>29.061972</td>\n",
       "      <td>14.105166</td>\n",
       "      <td>7.139943</td>\n",
       "      <td>933.313064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>560.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.050000</td>\n",
       "      <td>1683.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13169.750</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.475000</td>\n",
       "      <td>3509.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25477.000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>22.815000</td>\n",
       "      <td>4175.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37446.250</td>\n",
       "      <td>73.250000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>28.850000</td>\n",
       "      <td>4863.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49997.000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>34.990000</td>\n",
       "      <td>6530.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Square Footage  Number of Occupants  Appliances Used  \\\n",
       "count        1000.000          1000.000000      1000.000000   \n",
       "mean        25462.388            48.372000        25.606000   \n",
       "std         14294.554            29.061972        14.105166   \n",
       "min           560.000             1.000000         1.000000   \n",
       "25%         13169.750            22.000000        13.000000   \n",
       "50%         25477.000            47.000000        26.000000   \n",
       "75%         37446.250            73.250000        38.000000   \n",
       "max         49997.000            99.000000        49.000000   \n",
       "\n",
       "       Average Temperature  Energy Consumption  \n",
       "count          1000.000000         1000.000000  \n",
       "mean             22.611390         4166.252570  \n",
       "std               7.139943          933.313064  \n",
       "min              10.050000         1683.950000  \n",
       "25%              16.475000         3509.482500  \n",
       "50%              22.815000         4175.730000  \n",
       "75%              28.850000         4863.850000  \n",
       "max              34.990000         6530.600000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe gives a summary of the numerical features in the dataframe\n",
    "energy_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c1b64-6187-4a21-9f8e-4dfefa563da7",
   "metadata": {},
   "source": [
    "EDA can be much more extensive, but for this exercise we will stop here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963723e6",
   "metadata": {},
   "source": [
    "Before we proceed, we will need to drop the `Building Type` and `Day of Week` features as these are categorical and we have not talked yet about handling features of different type. \n",
    "\n",
    "We will keep all the other (numerical) features and separate them from the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d864d06c-9f16-49b2-886c-1f03baf0c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unused features, and separating features and target\n",
    "\n",
    "X = energy_df.drop(columns = ['Building Type', 'Day of Week', 'Energy Consumption'])\n",
    "y = energy_df['Energy Consumption']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8164b4bc-41b4-4b3f-9160-f967d039e37e",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "As discussed in class, it is important for models to generalize to unseen data, therefore we will set aside a subset of samples to evaluate the model on samples it has not been trained on.\n",
    "\n",
    "This is a rather small dataset, so we will have to compromise to have enough data for training and testing. Saving 20% for testing sounds acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57071e10-9339-4d29-95f7-1d21a7a0e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b58dd9-42c1-4d23-a2b3-cae4416af167",
   "metadata": {},
   "source": [
    "## <font color='red'>Question 3: Baseline model</font>\n",
    "\n",
    "As always, we will start by building a baseline model to use as reference. Build and score your model in the cell below. Remember that this is a *regression* problem, so you will not use `DummyClassifier`, but the corresponding model for regression!\n",
    "\n",
    "Also, remember to score the model on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2ba1572-9cb8-47f9-b3a3-89a812eb705d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02952458891450549"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.dummy import DummyRegressor\n",
    "dt = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "dt.score(X_train, y_train)\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c0863-af21-4286-8942-435f5acb69f6",
   "metadata": {},
   "source": [
    "**Note:** did you check the value of a prediction from the baseline model? It is very close to the mean of `Energy Consumption`, as expected! But not exactly the same, as we set some samples aside for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e5f3c-bf46-46df-b6f5-82d69524c489",
   "metadata": {},
   "source": [
    "## <font color='red'>Question 4: Decision tree</font>\n",
    "\n",
    "Let's now try a more sofisticated approach. We will use a `DecisionTreeRegressor` to predict the energy consumption of a building. Run the code below and answer the following questions:\n",
    "- Why is there a large gap between train and test scores?\n",
    "- What would be the effect of increasing or decreasing test_size? How would that affect your confidence in the test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8567d777-f497-4fd8-94b5-2156941b2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a class object \n",
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "\n",
    "# Train a decision tree on X_train, y_train\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Score on the train set\n",
    "dt.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386cd71-c772-4cc0-8760-b4ba0b27bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on the test set\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ce3a1-2e61-4f99-bfc2-f6e06040f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are curious, you can see the depth of this tree - what do you think of this value? What does it tell us?\n",
    "dt.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0cdae-a7e1-45fb-ad32-4c7caae077f3",
   "metadata": {},
   "source": [
    "## <font color='red'>Question 5: Hyperparameter tuning</font>\n",
    "\n",
    "The model above is showing clear signs of **overfitting:** it learned *too much* from the training set, including noise and errors, and that has a negative impact on its ability to predict unseen samples. To fix his, we are going to force the tree to *learn less* by reducing its maximum depth.\n",
    "\n",
    "As we search for the best depth for our tree, we need to further split the training set in training and validation, to avoid using the test set for hyperparameter tuning (which means breaking the golden rule - the test set can not influence the model in any way, not even in the choice of hyperparameter).\n",
    "\n",
    "Let's move 20% of samples from the training set to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb2cc6-08af-410a-a04d-db9ffb68b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set \n",
    "X_tr, X_valid, y_tr, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ddad4-1565-4b37-99c3-356843555462",
   "metadata": {},
   "source": [
    "Now, we will try different depth values and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dbe362-4cb2-45d2-b247-8efd11843b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_scores = []\n",
    "valid_scores = []\n",
    "depths = np.arange(1, 35, 2)\n",
    "\n",
    "for depth in depths:  \n",
    "    # Create and fit a decision tree model for the given depth  \n",
    "    dt = DecisionTreeRegressor(max_depth=depth, random_state=123)\n",
    "\n",
    "    dt.fit(X_tr, y_tr)\n",
    "    # Calculate and append r2 scores on the training and validation sets\n",
    "    tr_scores.append(dt.score(X_tr, y_tr))    \n",
    "    valid_scores.append(dt.score(X_valid, y_valid))\n",
    "    \n",
    "results_single_valid_df = pd.DataFrame({\"train_score\": tr_scores, \n",
    "                           \"valid_score\": valid_scores},index = depths)\n",
    "results_single_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec4be0-9cbb-4406-8780-706409c4c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results above\n",
    "results_single_valid_df[['train_score', 'valid_score']].plot(ylabel='r2 scores');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a277140-2a82-4b50-aab9-e340adb0caa0",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "- What is the best tree depth? How did you choose this value?\n",
    "- How would you describe gap between training and validation set for smaller depth values? And what about higher values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595af60c-6e6a-476d-856e-9f64b7ee103c",
   "metadata": {},
   "source": [
    "## <font color='red'>Question 5: Cross-validation</font>\n",
    "\n",
    "Our validation set is quite small - only 160 samples. This means that it could allow for some significant variance in the score if a different set was picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117382a-f74f-491d-bafa-e04e02a09a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the size of the validation set\n",
    "X_valid.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee3af4-792c-43de-9e14-aa3051d1223d",
   "metadata": {},
   "source": [
    "In the cell below, we are going to observe this phenomenon, by using `cross_validate` on our best tree candidate (`max_depth` = 5). See how the test_scores change with every different fold?\n",
    "\n",
    "**Notes:** \n",
    "- `cross_validate` has no concept of validation set, and it calls it test set instead. For our purposes, test_scores are validation scores\n",
    "- Because we are using cross-validation, we can use the original X_train set, before we further divided it in training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2b887-7f9e-4236-b1d5-bb51913a9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best = DecisionTreeRegressor(max_depth=5, random_state=123)\n",
    "\n",
    "scores = cross_validate(dt_best, X_train, y_train, cv=10, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4e986-3b07-42a4-add5-fed00cca68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of above values\n",
    "pd.DataFrame(pd.DataFrame(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553fc04-8b64-4f8a-86c7-05a4c4afac8f",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "- What is the highest validation score? And lowest? How far are they from the mean value? Would it have been possible for us to see any of these scores if we used only one validation set?\n",
    "- How did cross-validation help us getting a more robust score measure?\n",
    "- Fold 6 has the best validation score. Shouldn't we just use the model fitted on this particular training fold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b5988-880b-4352-aeef-f1df66dbf551",
   "metadata": {},
   "source": [
    "#### Final note\n",
    "\n",
    "Cross-validation is often used in the context of hyperparameter tuning, such as in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cada5-c241-4d66-9d97-b5cb293b426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(1, 35, 2)\n",
    "\n",
    "cv_train_scores = []\n",
    "cv_valid_scores = []\n",
    "for depth in depths: \n",
    "    # Create and fit a decision tree model for the given depth   \n",
    "    dt = DecisionTreeRegressor(max_depth = depth, random_state=123)\n",
    "\n",
    "    # Carry out cross-validation\n",
    "    results = cross_validate(dt, X_train, y_train, cv=5, return_train_score=True)\n",
    "    cv_train_scores.append(results['train_score'].mean())\n",
    "    cv_valid_scores.append(results['test_score'].mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ba26d-d68a-4c19-bcd5-17365a8eae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\"train_score\": cv_train_scores, \n",
    "                           \"valid_score\": cv_valid_scores\n",
    "                           },\n",
    "                           index=depths\n",
    "                            )\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad2917-7282-4d66-8146-a6159d3bb30a",
   "metadata": {},
   "source": [
    "Thanks to cross-validation, the validation scores that you see in this example are more stable than the scores one could obtain using a single validation set (and again, 5 seems to be the best depth for this problem).\n",
    "\n",
    "**The purpose of cross-validation, however, is not hyperparameter tuning.** Cross-validation does not give us the best hyperparameters for the model. It produces a more robust score for a specific model and a specific set of hyperparameters, set by us.\n",
    "\n",
    "**Because they are often seen together, people can mistake cross-validation and hyperparameter tuning as being the same thing.** But because you did this tutorial, you will not get confused anymore ðŸ˜Š."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb095c47-1948-453e-93b2-63a7383e2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step: final training and scoring on test set. Pick the correct sets for this task!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f535f-c243-4187-82e5-57d27ec13108",
   "metadata": {},
   "source": [
    "## <font color='red'>Question 6: Hyperparameters playground</font>\n",
    "\n",
    "We are now going to look at a different problem - a classification one - to see the impact on different hyperparameters on model learning.\n",
    "\n",
    "In this interactive playground, you will investigate how various algorithms create decision boundaries to distinguish between Iris flower species using their sepal length and width as features. By adjusting the parameters, you can observe how the decision boundaries change, which can result in either overfitting (where the model fits the training data too closely) or underfitting (where the model is too simplistic).\n",
    "\n",
    "- With **k-Nearest Neighbours ($k$-NN)**, you'll determine how many neighboring flowers to consult. Should we rely on a single nearest neighbor? Or should we consider a wider group? \n",
    "\n",
    "- With **Support Vector Machine (SVM)** using the RBF kernel, you'll tweak the hyperparameters `C` and `gamma` to explore the tightrope walk between overly complex boundaries (that might overfit) and overly broad ones (that might underfit).\n",
    "  \n",
    "- With **Decision trees**, you'll observe the effect of `max_depth` on the decision boundary. \n",
    "\n",
    "Observe the process of crafting and refining decision boundaries, one parameter at a time! Be sure to take breaks to reflect on the results you are observing, and answer the following questions:\n",
    "\n",
    "- For each hyperparameter, write down the relationship between value and model complexity (does the complexity increase with the value or vice-versa?).\n",
    "- What hyperparameter value (or combination of values) seems to give the best results for each model? Is this problem better solved by complex models, or simpler ones? **Hint:** the dataset is small, which increases the risk of overfitting if we pick too complex models.\n",
    "- Describe the appearance of the decision boundaries for each model. Which model presents as smooth, curved lines? Which one looks like a very fragmented line? Note that the appearance will vary as you change the hyperparameter values, but you should be able to spot some common patterns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769f768-a3c3-4d61-94a7-bedb98b6d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "\n",
    "import panel as pn\n",
    "from panel import widgets\n",
    "from panel.interact import interact\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from ipywidgets import interact, FloatLogSlider, IntSlider\n",
    "import mglearn\n",
    "\n",
    "\n",
    "# Load dataset and preprocessing\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.data\n",
    "iris_df['species'] = iris.target\n",
    "iris_df = iris_df[iris_df['species'] > 0]\n",
    "X, y = iris_df[['sepal length (cm)', 'sepal width (cm)']], iris_df['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=123)\n",
    "\n",
    "\n",
    "# Common plot settings\n",
    "def plot_results(model, X_train, y_train, title, ax):\n",
    "    mglearn.plots.plot_2d_separator(model, X_train.values, fill=True, alpha=0.4, ax=ax);\n",
    "    mglearn.discrete_scatter(\n",
    "        X_train[\"sepal length (cm)\"], X_train[\"sepal width (cm)\"], y_train, s=6, ax=ax\n",
    "    );\n",
    "    ax.set_xlabel(\"sepal length (cm)\", fontsize=12);\n",
    "    ax.set_ylabel(\"sepal width (cm)\", fontsize=12);\n",
    "    train_score = np.round(model.score(X_train.values, y_train), 2)\n",
    "    test_score = np.round(model.score(X_test.values, y_test), 2)\n",
    "    ax.set_title(\n",
    "        f\"{title}\\n train score = {train_score}\\ntest score = {test_score}\", fontsize=8\n",
    "    );\n",
    "    pass\n",
    "\n",
    "\n",
    "# Widgets for SVM, k-NN, and Decision Tree\n",
    "c_widget = pn.widgets.FloatSlider(\n",
    "    value=1.0, start=1, end=5, step=0.1, name=\"C (log scale)\"\n",
    ")\n",
    "gamma_widget = pn.widgets.FloatSlider(\n",
    "    value=1.0, start=-3, end=5, step=0.1, name=\"Gamma (log scale)\"\n",
    ")\n",
    "n_neighbors_widget = pn.widgets.IntSlider(\n",
    "    start=1, end=40, step=1, value=5, name=\"n_neighbors\"\n",
    ")\n",
    "max_depth_widget = pn.widgets.IntSlider(\n",
    "    start=1, end=20, step=1, value=3, name=\"max_depth\"\n",
    ")\n",
    "\n",
    "\n",
    "# The update function to create the plots\n",
    "def update_plots(c, gamma=1.0, n_neighbors=5, max_depth=3):\n",
    "    c_log = round(10**c, 2)  # Transform C to logarithmic scale\n",
    "    gamma_log = round(10**gamma, 2)   # Transform Gamma to logarithmic scale\n",
    "\n",
    "    fig = Figure(figsize=(8, 2))\n",
    "    axes = fig.subplots(1, 3)\n",
    "\n",
    "    models = [\n",
    "        SVC(C=c_log, gamma=gamma_log, random_state=42),\n",
    "        KNeighborsClassifier(n_neighbors=n_neighbors),\n",
    "        DecisionTreeClassifier(max_depth=max_depth, random_state=42),\n",
    "    ]\n",
    "    titles = [\n",
    "        f\"SVM (C={c_log}, gamma={gamma_log})\",\n",
    "        f\"k-NN (n_neighbors={n_neighbors})\",\n",
    "        f\"Decision Tree (max_depth={max_depth})\",\n",
    "    ]\n",
    "    for model, title, ax in zip(models, titles, axes):\n",
    "        model.fit(X_train.values, y_train)\n",
    "        plot_results(model, X_train, y_train, title, ax);\n",
    "    # print(c, gamma, n_neighbors, max_depth)\n",
    "    return pn.pane.Matplotlib(fig, tight=True);\n",
    "\n",
    "\n",
    "# Bind the function to the panel widgets\n",
    "interactive_plot = pn.bind(\n",
    "    update_plots,\n",
    "    c=c_widget.param.value_throttled,\n",
    "    gamma=gamma_widget.param.value_throttled,\n",
    "    n_neighbors=n_neighbors_widget.param.value_throttled,\n",
    "    max_depth=max_depth_widget.param.value_throttled,\n",
    ")\n",
    "\n",
    "# Layout the widgets and the plot\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(c_widget, n_neighbors_widget),\n",
    "    pn.Row(gamma_widget, max_depth_widget),\n",
    "    interactive_plot,\n",
    ")\n",
    "\n",
    "# Display the interactive dashboard\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a499243-8b9c-4601-93a2-b133486f59ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
